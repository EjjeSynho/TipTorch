{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import pickle\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from tools.plotting import plot_radial_PSF_profiles, draw_PSF_stack\n",
    "from torchmin import minimize\n",
    "\n",
    "from project_settings import device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The procedure of converting an .ini file to config is quite intricate and implemented differently for each instrument. That's why we just load\n",
    "the config from the sample data in this example. The main idea is to convert all entries in the config into proper dimensionality so that \n",
    "TipTorch can understand it. For example, for check SPHERE_preprocess() and GetSPHEREonsky() functions (to be refactored in the future).\n",
    "Check the dimensions of the tensors in this code to define your custom config if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/samples/IRDIS_sample_data.pkl', 'rb') as handle:\n",
    "    sample_data = pickle.load(handle)\n",
    "\n",
    "config   = sample_data['config']\n",
    "PSF_data = sample_data['PSF']\n",
    "PSF_mask = sample_data['mask']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the PSF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PSF_models.TipTorch import TipTorch\n",
    "\n",
    "# Configure which error terms PSDs to include in the PSF model\n",
    "PSD_include = {\n",
    "    'fitting':         True,  # fitting error\n",
    "    'WFS noise':       True,  # wavefront sensor noise\n",
    "    'spatio-temporal': True,  # spatio-temporal errors\n",
    "    'aliasing':        True,  # aliasing error\n",
    "    'chromatism':      True,  # chromatic effects\n",
    "    'diff. refract':   True,  # differential refraction error\n",
    "    'Moffat':          False  # Moffat \"absorber\" PSD. Switched off for this example\n",
    "}\n",
    "\n",
    "# Initialize the TipTorch PSF model with the loaded configuration\n",
    "model = TipTorch(\n",
    "    AO_config    = config, # configuration parameters dictionary\n",
    "    AO_type      = 'SCAO',      # selected AO mode\n",
    "    pupil        = None,        # using default pupil (and apodizer) defined in config\n",
    "    PSD_include  = PSD_include, # which error terms to include\n",
    "    norm_regime  = 'sum',       # normalize PSFs to sum = 1 over the PSF\n",
    "    device       = device,      # device to run computations on (CPU or GPU)\n",
    "    oversampling = 1            # oversampling factor\n",
    ")\n",
    "\n",
    "# In float regime, the model is faster and only marginally less accurate, so recommended\n",
    "model.to_float()\n",
    "\n",
    "# Running model with the parameters defined in the config. Values unspecifiiied in the config are just set to default values\n",
    "PSF_test = model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For practical reason, static phase in the pupil plane is managed externaly and not included in the model. However, this can be done by adding a new input to the model. For example, we can add Zernike-driven static aberrations map. In addition to this, we can include quasi-static modes associated with Low Wind Effect (LWE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.static_phase import ZernikeBasis, LWEBasis\n",
    "\n",
    "# Pupil iis managed inside the PSF model, so we need to ignore it in the basis computation\n",
    "LWE_basis = LWEBasis(model=model, ignore_pupil=True)\n",
    "Z_basis = ZernikeBasis(model=model, N_modes=300, ignore_pupil=True)\n",
    "\n",
    "# Compute static phase from the modal coefficients\n",
    "def compute_static_phase(input_dict):\n",
    "    return Z_basis(input_dict['Z_coefs']) * LWE_basis(input_dict['LWE_coefs']) * model.pupil * model.apodizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, pytorch-minimize is used to optimize the model inputs. The inputs are managed by the InputsManager class. # InputsManager allows to normalize model inputs that mau take ildly different ranges of values. In addition, it allows to  easily stack inputs dictionary to a single vector/matrix for optimization and unstack it back to a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_processing.normalizers import Uniform\n",
    "from managers.input_manager import InputsManager\n",
    "\n",
    "\n",
    "model_inputs = InputsManager()\n",
    "'''\n",
    "Note, that it is possible to add parameters with arbitrary shapes and names into the manager.\n",
    "The only ones which named the same as internakl variables of the model will be used in the model.\n",
    "Other parameters then must be handled manually by the user, like Z_coefs and LWE_coefs in this example.\n",
    "'''\n",
    "# The dimensionality of inputs is very important, since PSF model doesn't do any checking itself\n",
    "model_inputs.add('r0',  model.r0,                 Uniform(a=0.05,  b=0.5))\n",
    "model_inputs.add('F',   torch.tensor([[1.0,]*2]), Uniform(a=0.0,   b=1.0))\n",
    "model_inputs.add('dx',  torch.tensor([[0.0,]*2]), Uniform(a=-1,    b=1))\n",
    "model_inputs.add('dy',  torch.tensor([[0.0,]*2]), Uniform(a=-1,    b=1))\n",
    "model_inputs.add('bg',  torch.tensor([[0.0,]*2]), Uniform(a=-5e-6, b=5e-6))\n",
    "model_inputs.add('dn',  torch.tensor([0.0]),      Uniform(a=-0.02, b=0.02))\n",
    "model_inputs.add('Jx',  torch.tensor([[7.5]]),    Uniform(a=0,     b=40))\n",
    "model_inputs.add('Jy',  torch.tensor([[7.5]]),    Uniform(a=0,     b=40))\n",
    "model_inputs.add('Jxy', torch.tensor([[18]]),     Uniform(a=-180,  b=180))\n",
    "\n",
    "model_inputs.add('LWE_coefs', torch.zeros([1,12]),    Uniform(a=-20,   b=20))\n",
    "model_inputs.add('Z_coefs',   torch.zeros([1, Z_basis.N_modes]), Uniform(a=-10,   b=10))\n",
    "\n",
    "model_inputs.to_float()\n",
    "model_inputs.to(device)\n",
    "\n",
    "print(model_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function below simulates the PSF given the inputs stacked into a single vector. Used in the optimization process. Given this vector, it is correct to say that model is fully defined by:\n",
    "  * the internal values pre-set with config during the initialization,\n",
    "  * the input dictionary which overloads some of these parameters every time this function is called,\n",
    "  * the external phase generator which is also called every time this function is called.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate(x_):\n",
    "    # Note, that every call to model_inputs.unstack() will update the internal state of model_inputs\n",
    "    # Switching the update off helps to leave the internal state of model_inputs intact\n",
    "    input_dict = model_inputs.unstack(x_, update=True)\n",
    "    # PSD = None means that the PSD will be computed inside the model and not provided from outside.\n",
    "    return model(x=input_dict, PSD=None, phase_generator=lambda: compute_static_phase(input_dict))\n",
    "\n",
    "x0 = model_inputs.stack()\n",
    "\n",
    "# Direct prediction without any calibration or fitting, quite inaccurate\n",
    "PSF_pred = simulate(x0)\n",
    "\n",
    "draw_PSF_stack(PSF_data*PSF_mask, PSF_pred*PSF_mask, average=True, min_val=1e-5, crop=80, scale='log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only purpose of this class is to regularize the optimization of LWE coefficients to avoid overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LWERegularizer:\n",
    "    # These initial values are completely empirical\n",
    "    def __init__(self, device, amplitude=50.0, gaussian_sigma_factor=2.0, gauss_penalty_weight=5.0, l2_weight=1e-4):\n",
    "        self.device = device\n",
    "        self.amplitude = amplitude\n",
    "        self.gaussian_sigma = amplitude / gaussian_sigma_factor\n",
    "        self.l2_weight = l2_weight\n",
    "        self.gauss_penalty_weight = gauss_penalty_weight\n",
    "\n",
    "        # Define patterns to avoid while optimizing because they are unlikely to appear physically and thus may mean overfitting\n",
    "        pattern_templates = [\n",
    "            [0,0,0,0,  0,-1,1,0,  1,0,0,-1],  # pattern_outwards\n",
    "            [0,0,0,0,  0,1,-1,0, -1,0,0, 1],  # pattern_inwards\n",
    "            [0,0,0,0,  0,-1,1,0, -1,0,0, 1],  # pattern_1\n",
    "            [0,0,0,0,  0,1,-1,0,  1,0,0,-1],  # pattern_2\n",
    "            [0,0,0,0,  1,0,0,-1,  0,1,-1,0],  # pattern_3\n",
    "            [0,0,0,0,  -1,0,0,1,  0,-1,1,0],  # pattern_4\n",
    "            [-1,1,1,-1,  0,0,0,0,  0,0,0,0],  # pattern_piston_horiz\n",
    "            [1,-1,-1,1,  0,0,0,0,  0,0,0,0]   # pattern_piston_vert\n",
    "        ]\n",
    "\n",
    "        # Create tensor patterns from templates\n",
    "        self.patterns = [torch.tensor([p]).to(device).float() * self.amplitude for p in pattern_templates]\n",
    "\n",
    "    def gaussian_penalty(self, amplitude, x, x_0, sigma):\n",
    "        # Calculate Gaussian penalty between coefficient vector and pattern template\n",
    "        return amplitude * torch.exp(-torch.sum((x - x_0) ** 2) / (2 * sigma ** 2))\n",
    "\n",
    "    def pattern_error(self, pattern, coefficients):\n",
    "        # Calculate error term for a specific pattern\n",
    "        return (pattern * self.gaussian_penalty(self.gauss_penalty_weight, coefficients, pattern, self.gaussian_sigma)).flatten().abs().sum()\n",
    "\n",
    "    def __call__(self, coefficients):\n",
    "        # Calculate the full LWE regularization loss for given coefficients\n",
    "        pattern_loss = sum(self.pattern_error(pattern, coefficients) for pattern in self.patterns)\n",
    "        # L2 regularization\n",
    "        LWE_l2_loss = (coefficients**2).mean() * self.l2_weight\n",
    "\n",
    "        return pattern_loss + LWE_l2_loss\n",
    "\n",
    "\n",
    "# Initialize the regularizer\n",
    "LWE_regularizer = LWERegularizer(device)\n",
    "\n",
    "L1_loss_custom = lambda x: ( (simulate(x)-PSF_data) * PSF_mask ).flatten().abs().sum()\n",
    "\n",
    "def loss_fn(x_):\n",
    "    # You can also update the models_inputs entries directly if needed\n",
    "    return L1_loss_custom(x_) + LWE_regularizer(model_inputs['LWE_coefs']) * float(model_inputs.is_optimizable('LWE_coefs'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do fitting. For this, the optimized model inputs must be first stacked into to a single vector, which is then passassed to the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch off optimization of Zernike coefficients for now, so they are not stacked into the model inputs vector\n",
    "model_inputs.set_optimizable('Z_coefs', False) # Can be done for any variable\n",
    "\n",
    "x0 = model_inputs.stack()\n",
    "result = minimize(loss_fn, x0, max_iter=300, tol=1e-5, method='l-bfgs', disp=2)\n",
    "x0 = result.x\n",
    "\n",
    "# Simulate PSF with updated parameters after the fitting\n",
    "PSF_fitted = simulate(x0)\n",
    "\n",
    "# You back up internal state of the inputs manager\n",
    "model_inputs_backup = model_inputs.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RenderPSFs(PSF_0, PSF_1, mask=1):\n",
    "    # Since SPHERE (IRDIS) PSFs have two spectral/polarization channels (L and R), both are rendered simultaneosuly\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10, 3))\n",
    "    # Draw profiles\n",
    "    plot_radial_PSF_profiles( (PSF_0*mask)[:,0,...].cpu().numpy(), (PSF_1*mask)[:,0,...].cpu().numpy(), 'Data', 'TipTorch', title='Left PSF',  ax=ax[0] )\n",
    "    plot_radial_PSF_profiles( (PSF_0*mask)[:,1,...].cpu().numpy(), (PSF_1*mask)[:,1,...].cpu().numpy(), 'Data', 'TipTorch', title='Right PSF', ax=ax[1] )\n",
    "    plt.show()\n",
    "    # Draw images\n",
    "    draw_PSF_stack(PSF_0*mask, PSF_1*mask, min_val=1e-6, average=True, crop=80)\n",
    "\n",
    "RenderPSFs(PSF_data, PSF_fitted, PSF_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's add Zernike modes just because"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs['Z_coefs'][..., 4] = 200 # [nm]\n",
    "model_inputs['Z_coefs'][..., 2] = 50  # [nm]\n",
    "\n",
    "PSF_with_defocus = simulate(x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RenderPSFs(PSF_data, PSF_with_defocus, PSF_mask)\n",
    "plt.show()\n",
    "\n",
    "OPD_map = Z_basis.compute_OPD(model_inputs['Z_coefs'])[0].detach().cpu().numpy() * 1e9 # [nm]\n",
    "plt.imshow(OPD_map, cmap='RdBu')\n",
    "plt.colorbar()\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AO-torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
